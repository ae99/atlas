{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.distributed as dist\n",
    "\n",
    "from src import dist_utils, slurm, util\n",
    "from src.index_io import load_or_initialize_index, save_embeddings_and_index\n",
    "from src.model_io import create_checkpoint_directories, load_or_initialize_atlas_model\n",
    "from src.options import get_options\n",
    "from src.tasks import get_task\n",
    "from src.index import DistributedFAISSIndex, DistributedIndex\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = get_options()\n",
    "\n",
    "opt = options.parse([\n",
    "    '--name', 'eval-test',\n",
    "    '--generation_max_length', '16',\n",
    "    '--gold_score_mode', 'pdist',\n",
    "    '--precision', 'fp32',\n",
    "    '--reader_model_type', 'google/t5-base-lm-adapt',\n",
    "    '--text_maxlength', '128',\n",
    "    '--model_path', '/atlas-data/models/atlas_nq/base/',\n",
    "    '--per_gpu_batch_size', '2',\n",
    "    '--n_context', '5',\n",
    "    '--retriever_n_context', '5',\n",
    "    '--checkpoint_dir', '/atlas-data/checkpoints',\n",
    "    '--main_port', '12345',\n",
    "    '--index_mode', 'flat',\n",
    "    '--task', 'qa',\n",
    "    '--write_results',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_data_iterator(questions): \n",
    "    for question in questions:\n",
    "        yield question\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, index, opt, questions, step=None):\n",
    "    model.eval()\n",
    "    metrics = defaultdict(lambda: [])\n",
    "    dataset_wpred = []\n",
    "    unwrapped_model = util.get_unwrapped_model_if_wrapped(model)\n",
    "    reader_tokenizer = unwrapped_model.reader_tokenizer\n",
    "\n",
    "    task = get_task(opt, reader_tokenizer)\n",
    "    data_iterator = qa_data_iterator(questions)\n",
    "    data_iterator = filter(None, map(task.process, data_iterator))\n",
    "    data_iterator = list(task.batch_iterator(data_iterator, opt.per_gpu_batch_size))\n",
    "\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        query = batch.get(\"query\", [\"\"])\n",
    "        answers = batch.get(\"target\", [\"\"])\n",
    "        batch_metadata = batch.get(\"metadata\")\n",
    "        target_tokens = batch.get(\"target_tokens\")\n",
    "\n",
    "        query_enc, labels, decoder_input_ids = unwrapped_model.tokenize(query, answers, target_tokens=target_tokens)\n",
    "\n",
    "        query_ids_retriever = query_enc[\"input_ids\"].cuda()\n",
    "        query_mask_retriever = query_enc[\"attention_mask\"].cuda()\n",
    "        retrieved_passages, _ = unwrapped_model.retrieve(\n",
    "            index,\n",
    "            opt.n_context,\n",
    "            query,\n",
    "            query_ids_retriever,\n",
    "            query_mask_retriever,\n",
    "            batch_metadata=batch_metadata,\n",
    "            filtering_fun=task.filter,\n",
    "        )\n",
    "\n",
    "        reader_tokens, _ = unwrapped_model.tokenize_passages(query, retrieved_passages)\n",
    "\n",
    "\n",
    "        if \"eval_loss\" in task.metrics:\n",
    "            eval_loss, logits = unwrapped_model.compute_reader_loss_and_logits(reader_tokens, decoder_input_ids, labels)\n",
    "            metrics[\"eval_loss\"].append(eval_loss)\n",
    "\n",
    "        generation = unwrapped_model.generate(\n",
    "            reader_tokens, query, choices=batch[\"choices\"] if \"choices\" in batch else None\n",
    "        )\n",
    "\n",
    "        for k, g in enumerate(generation):\n",
    "            if opt.decoder_prompt_format is not None:\n",
    "                query_ids = reader_tokenizer.encode(\n",
    "                    opt.decoder_prompt_format.format_map({\"query\": query[k]}), add_special_tokens=False\n",
    "                )\n",
    "                g = g[len(query_ids) + 1 :]\n",
    "            pred = reader_tokenizer.decode(g, skip_special_tokens=True)\n",
    "            gold = [answers[k]] if not \"answers\" in batch else batch[\"answers\"][k]\n",
    "            sample_metrics = task.evaluation(pred, gold)\n",
    "            for key, value in sample_metrics.items():\n",
    "                metrics[key].append(value)\n",
    "\n",
    "            ex = {\"query\": query[k], \"answers\": gold, \"generation\": pred}\n",
    "            if not opt.dont_write_passages:\n",
    "                ex[\"passages\"] = retrieved_passages[k]\n",
    "            if batch_metadata is not None:\n",
    "                ex[\"metadata\"] = batch_metadata[k]\n",
    "            if opt.task == \"multiple_choice\":\n",
    "                ex[\"choice_logits\"] = task.get_choice_logits(logits[k])\n",
    "            if \"id\" in batch:\n",
    "                ex[\"id\"] = batch[\"id\"][k]\n",
    "            dataset_wpred.append(ex)\n",
    "\n",
    "    metrics, dataset_wpred = task.evaluation_postprocessing(metrics, dataset_wpred)\n",
    "    metrics = util.avg_dist_dict(task.metrics, metrics)\n",
    "    metrics = {key: value if key == \"eval_loss\" else 100 * value for key, value in metrics.items()}\n",
    "\n",
    "    print('results', dataset_wpred)\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/30/2022 12:48:10] {model_io.py:130} INFO - Loading /atlas-data/models/atlas_nq/base\n",
      "[11/30/2022 12:48:10] {model_io.py:131} INFO - loading checkpoint /atlas-data/models/atlas_nq/base/model.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/contriever were not used when initializing Contriever: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing Contriever from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Contriever from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/30/2022 12:48:16] {atlas.py:53} INFO - Atlas Init\n",
      "[11/30/2022 12:48:16] {model_io.py:194} INFO - Model loaded from /atlas-data/models/atlas_nq/base/\n",
      "[11/30/2022 12:48:16] {atlas.py:84} INFO - 10 passages encoded on process: 0\n",
      "hey torch.Size([2, 640]) False\n",
      "True torch.Size([10, 128, 768]) torch.Size([2, 640, 768])\n",
      "hey torch.Size([2, 512]) True\n",
      "decoder tensor([[    0, 32099,  6154,  ...,     0,     0,     0],\n",
      "        [    0, 32099,     3,  ...,     0,     0,     0]], device='cuda:0')\n",
      "hey torch.Size([2, 640]) False\n",
      "True torch.Size([10, 128, 768]) torch.Size([2, 640, 768])\n",
      "hey torch.Size([2, 1]) True\n",
      "decoder tensor([[0],\n",
      "        [0]], device='cuda:0')\n",
      "hey torch.Size([2, 1]) True\n",
      "decoder tensor([[32099],\n",
      "        [32099]], device='cuda:0')\n",
      "hey torch.Size([2, 1]) True\n",
      "decoder tensor([[6154],\n",
      "        [   3]], device='cuda:0')\n",
      "hey torch.Size([2, 1]) True\n",
      "decoder tensor([[3769],\n",
      "        [7422]], device='cuda:0')\n",
      "hey torch.Size([2, 1]) True\n",
      "decoder tensor([[    1],\n",
      "        [15927]], device='cuda:0')\n",
      "results [{'query': 'question: What is my favourite number? answer: <extra_id_0>', 'answers': ['<extra_id_0> 3455'], 'generation': '3455', 'passages': [{'title': 'My favourite number', 'text': 'My favourite number is 3455'}, {'title': 'My favourite number', 'text': 'My favourite number is 3455'}, {'title': 'My favourite number', 'text': 'My favourite number is 3455'}, {'title': 'My favourite number', 'text': 'My favourite number is 3455'}, {'title': 'My favourite number', 'text': 'My favourite number is 3455'}], 'metadata': {}}, {'query': 'question: What is the secret word? answer: <extra_id_0>', 'answers': ['<extra_id_0> FROG'], 'generation': 'FROG', 'passages': [{'title': 'The secret word', 'text': 'The secret word is FROG'}, {'title': 'The secret word', 'text': 'The secret word is FROG'}, {'title': 'The secret word', 'text': 'The secret word is FROG'}, {'title': 'The secret word', 'text': 'The secret word is FROG'}, {'title': 'The secret word', 'text': 'The secret word is FROG'}], 'metadata': {}}]\n",
      "0.007305862847715616\n"
     ]
    }
   ],
   "source": [
    "passegesEmpty = [{\n",
    "    'title': '',\n",
    "    'text': ''\n",
    "}] * 5\n",
    "\n",
    "passagesCorrect = [{\n",
    "    'title': 'My favourite number',\n",
    "    'text': 'My favourite number is 3455'\n",
    "}, {\n",
    "    'title': 'The secret word',\n",
    "    'text': 'The secret word is FROG'\n",
    "}] * 5\n",
    "\n",
    "passagesIncorrect = [{\n",
    "    'title': 'My favourite number',\n",
    "    'text': 'My favourite number is 290'\n",
    "}, {\n",
    "    'title': 'The secret word',\n",
    "    'text': 'The secret word is CAR'\n",
    "}] * 5\n",
    "\n",
    "questions = [{\n",
    "    'question': 'What is my favourite number?',\n",
    "    'target': '3455',  \n",
    "}, {\n",
    "    'question': 'What is the secret word?',\n",
    "    'target': 'FROG',\n",
    "}]\n",
    "\n",
    "\n",
    "logger = util.init_logger()\n",
    "torch.manual_seed(opt.seed)\n",
    "\n",
    "opt.device = 'cuda'\n",
    "opt.global_rank = 0\n",
    "opt.world_size = 1\n",
    "opt.is_distributed = False\n",
    "opt.is_main = True\n",
    "\n",
    "index = DistributedIndex()\n",
    "passages = passagesCorrect\n",
    "\n",
    "index.init_embeddings(passages)\n",
    "model, _, _, _, _, opt, step = load_or_initialize_atlas_model(opt, eval_only=True)\n",
    "\n",
    "model.build_index(index, passages, opt.per_gpu_embedder_batch_size, logger)\n",
    "metrics = evaluate(model, index, opt, questions, step)\n",
    "print(metrics['eval_loss'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7026, 0.0875, 0.1418, 0.8254, 0.6902],\n",
       "         [0.3329, 0.5169, 0.5192, 0.7989, 0.4210],\n",
       "         [0.8796, 0.5450, 0.0854, 0.7404, 0.3570],\n",
       "         [0.8371, 0.2315, 0.3826, 0.0085, 0.6328]],\n",
       "\n",
       "        [[0.8045, 0.1771, 0.8830, 0.7865, 0.6945],\n",
       "         [0.7434, 0.3773, 0.0839, 0.5978, 0.9541],\n",
       "         [0.3975, 0.5133, 0.0864, 0.3684, 0.9206],\n",
       "         [0.7616, 0.2692, 0.6412, 0.1603, 0.8845]],\n",
       "\n",
       "        [[0.4705, 0.6577, 0.2136, 0.7203, 0.5471],\n",
       "         [0.5106, 0.9789, 0.9993, 0.2279, 0.9662],\n",
       "         [0.8593, 0.8149, 0.6101, 0.6662, 0.4376],\n",
       "         [0.6804, 0.7272, 0.1327, 0.2583, 0.0055]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                # last_hidden_state = output.last_hidden_state\n",
    "                # output.last_hidden_state = last_hidden_state.view(self.config.bsz, -1, last_hidden_state.size(-1))\n",
    "                # print(return_dict, last_hidden_state.size(), output.last_hidden_state.size())\n",
    "# True torch.Size([10, 128, 768]) torch.Size([2, 640, 768])\n",
    "\n",
    "\n",
    "#                                    (batch_size * num_passages, seq_len, hidden_size)\n",
    "# output.last_hidden_state starts as [10, 128, 768]\n",
    "# then we go ahead and *unstack* it.\n",
    "# gets converted to [2, 640, 768]\n",
    "#             (batch_size, seq_len * num_passages, hidden_size)\n",
    "a = torch.rand(3,4,5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15])\n",
      "torch.Size([10, 3])\n",
      "tensor([[0.7006, 0.2428, 0.0318, 0.5096, 0.2905, 0.6130, 0.6844, 0.3915, 0.3460,\n",
      "         0.1144, 0.5269, 0.6296, 0.4549, 0.4846, 0.6101],\n",
      "        [0.0440, 0.7096, 0.7897, 0.5727, 0.0889, 0.1923, 0.4750, 0.7143, 0.3561,\n",
      "         0.0122, 0.7269, 0.5587, 0.8398, 0.2020, 0.4995]])\n",
      "tensor([[0.7006, 0.2428, 0.0318],\n",
      "        [0.5096, 0.2905, 0.6130],\n",
      "        [0.6844, 0.3915, 0.3460],\n",
      "        [0.1144, 0.5269, 0.6296],\n",
      "        [0.4549, 0.4846, 0.6101],\n",
      "        [0.0440, 0.7096, 0.7897],\n",
      "        [0.5727, 0.0889, 0.1923],\n",
      "        [0.4750, 0.7143, 0.3561],\n",
      "        [0.0122, 0.7269, 0.5587],\n",
      "        [0.8398, 0.2020, 0.4995]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  (batch_size, seq_len * num_passages) to (batch_size * num_passages, seq_len)\n",
    "# a: torch.Size([2, 640]) -> b torch.Size([10, 128])\n",
    "# a: torch.Size([2, 15]) -> b torch.Size([10, 3])\n",
    "\n",
    "# random \"a\"\n",
    "a = torch.rand(2, 15)\n",
    "print(a.size())\n",
    "b = a.view(a.size(0) * 5, -1)\n",
    "# b = a.view(bs * num_passages, -1)\n",
    "print(b.size())\n",
    "print(a)\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('3.10.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b66dbdb614bfd022b649975f993762ad232399f75e8f30cb9176d485ed5b8487"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
